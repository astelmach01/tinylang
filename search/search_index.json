{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Tinylang","text":"<p>Tinylang is a Python library that provides a unified interface for interacting with various Large Language Models (LLMs) including OpenAI's GPT, Anthropic's Claude, and Google's Gemini.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Unified API for multiple LLM providers</li> <li>Support for OpenAI, Anthropic Claude, and Google Gemini</li> <li>Synchronous and asynchronous invocation methods</li> <li>Streaming support for real-time responses</li> <li>Chat history management</li> <li>Easy integration with existing projects</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install Tinylang:</p> <pre><code>pip install tinylang\n</code></pre> <p>Basic usage:</p> <pre><code>from tinylang.llms import ChatOpenAI\n\nchat = ChatOpenAI(\"gpt-3.5-turbo\")\nresponse = chat.invoke(\"Hello, how are you?\")\nprint(response)\n</code></pre> <p>For more detailed information, check out our Getting Started guide.</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>This page provides a comprehensive reference for the Tinylang API.</p>"},{"location":"api_reference/#chat-interfaces","title":"Chat Interfaces","text":""},{"location":"api_reference/#chatopenai","title":"ChatOpenAI","text":"<pre><code>class ChatOpenAI:\n    def __init__(self, model: str, api_key: Optional[str] = None, chat_history: int = 10, system_message: Optional[str] = None)\n    def invoke(self, prompt: str) -&gt; str\n    async def ainvoke(self, prompt: str) -&gt; str\n    def stream_invoke(self, prompt: str) -&gt; Iterator[str]\n    async def astream_invoke(self, prompt: str) -&gt; AsyncIterable[str]\n    def get_history(self) -&gt; List[Dict[str, str]]\n</code></pre>"},{"location":"api_reference/#chatclaude","title":"ChatClaude","text":"<pre><code>class ChatClaude:\n    def __init__(self, model: str, api_key: Optional[str] = None, chat_history: int = 10, system_message: Optional[str] = None)\n    def invoke(self, prompt: str) -&gt; str\n    async def ainvoke(self, prompt: str) -&gt; str\n    def stream_invoke(self, prompt: str) -&gt; Iterator[str]\n    async def astream_invoke(self, prompt: str) -&gt; AsyncIterable[str]\n    def get_history(self) -&gt; List[Dict[str, str]]\n</code></pre>"},{"location":"api_reference/#chatgemini","title":"ChatGemini","text":"<pre><code>class ChatGemini:\n    def __init__(self, model: str, api_key: Optional[str] = None, chat_history: int = 10, system_message: Optional[str] = None)\n    def invoke(self, prompt: str) -&gt; str\n    async def ainvoke(self, prompt: str) -&gt; str\n    def stream_invoke(self, prompt: str) -&gt; Iterator[str]\n    async def astream_invoke(self, prompt: str) -&gt; AsyncIterable[str]\n    def get_history(self) -&gt; List[Dict[str, str]]\n</code></pre>"},{"location":"api_reference/#chathistory","title":"ChatHistory","text":"<pre><code>class ChatHistory:\n    def __init__(self, max_history: int, system_message: str, previous_history: Optional[List[Dict[str, str]]] = None)\n    def add_message(self, role: str, content: str)\n    def get_messages() -&gt; List[Dict[str, str]]\n    def clear()\n</code></pre> <p>For detailed usage instructions and examples, please refer to the individual documentation pages for each class.</p>"},{"location":"chat_claude/","title":"ChatClaude","text":"<p>The <code>ChatClaude</code> class provides an interface to interact with Anthropic's Claude language models.</p>"},{"location":"chat_claude/#initialization","title":"Initialization","text":"<pre><code>from tinylang.llms import ChatClaude\n\nchat = ChatClaude(\n    model=\"claude-3-opus-20240229\",\n    api_key=None,  # Optional: Defaults to ANTHROPIC_API_KEY environment variable\n    chat_history=10,  # Optional: Number of messages to keep in history\n    system_message=\"You are a helpful assistant.\"  # Optional: System message for the conversation\n)\n</code></pre>"},{"location":"chat_claude/#methods","title":"Methods","text":""},{"location":"chat_claude/#invoke","title":"invoke","text":"<p>Synchronous method to get a response from the model.</p> <pre><code>response = chat.invoke(\"What is the capital of France?\")\nprint(response)\n</code></pre>"},{"location":"chat_claude/#ainvoke","title":"ainvoke","text":"<p>Asynchronous version of <code>invoke</code>.</p> <pre><code>import asyncio\n\nasync def get_response():\n    response = await chat.ainvoke(\"What is the capital of France?\")\n    print(response)\n\nasyncio.run(get_response())\n</code></pre>"},{"location":"chat_claude/#stream_invoke","title":"stream_invoke","text":"<p>Synchronous method that streams the response.</p> <pre><code>for chunk in chat.stream_invoke(\"Tell me a story\"):\n    print(chunk, end='', flush=True)\n</code></pre>"},{"location":"chat_claude/#astream_invoke","title":"astream_invoke","text":"<p>Asynchronous version of <code>stream_invoke</code>.</p> <pre><code>async def stream_response():\n    async for chunk in chat.astream_invoke(\"Tell me a story\"):\n        print(chunk, end='', flush=True)\n\nasyncio.run(stream_response())\n</code></pre>"},{"location":"chat_claude/#chat-history","title":"Chat History","text":"<p>The <code>ChatClaude</code> class automatically manages chat history. You can access the current history using:</p> <pre><code>history = chat.chat_history.get_messages()\nprint(history)\n</code></pre> <p>For more details on chat history management, see the Chat History documentation.</p>"},{"location":"chat_gemini/","title":"ChatGemini","text":"<p>The <code>ChatGemini</code> class provides an interface to interact with Google's Gemini language models.</p>"},{"location":"chat_gemini/#initialization","title":"Initialization","text":"<pre><code>from tinylang.llms import ChatGemini\n\nchat = ChatGemini(\n    model=\"gemini-1.5-pro\",\n    api_key=None,  # Optional: Defaults to GOOGLE_API_KEY environment variable\n    chat_history=10,  # Optional: Number of messages to keep in history\n    system_message=\"You are a helpful assistant.\"  # Optional: System message for the conversation\n)\n</code></pre>"},{"location":"chat_gemini/#methods","title":"Methods","text":""},{"location":"chat_gemini/#invoke","title":"invoke","text":"<p>Synchronous method to get a response from the model.</p> <pre><code>response = chat.invoke(\"What is the capital of France?\")\nprint(response)\n</code></pre>"},{"location":"chat_gemini/#ainvoke","title":"ainvoke","text":"<p>Asynchronous version of <code>invoke</code>.</p> <pre><code>import asyncio\n\nasync def get_response():\n    response = await chat.ainvoke(\"What is the capital of France?\")\n    print(response)\n\nasyncio.run(get_response())\n</code></pre>"},{"location":"chat_gemini/#stream_invoke","title":"stream_invoke","text":"<p>Synchronous method that streams the response.</p> <pre><code>for chunk in chat.stream_invoke(\"Tell me a story\"):\n    print(chunk, end='', flush=True)\n</code></pre>"},{"location":"chat_gemini/#astream_invoke","title":"astream_invoke","text":"<p>Asynchronous version of <code>stream_invoke</code>.</p> <pre><code>async def stream_response():\n    async for chunk in chat.astream_invoke(\"Tell me a story\"):\n        print(chunk, end='', flush=True)\n\nasyncio.run(stream_response())\n</code></pre>"},{"location":"chat_gemini/#chat-history","title":"Chat History","text":"<p>The <code>ChatGemini</code> class automatically manages chat history. You can access the current history using:</p> <pre><code>history = chat.chat_history.get_messages()\nprint(history)\n</code></pre> <p>For more details on chat history management, see the Chat History documentation.</p>"},{"location":"chat_history/","title":"Chat History","text":"<p>Tinylang provides built-in chat history management for all its chat interfaces. This allows for maintaining context across multiple interactions with the language models.</p>"},{"location":"chat_history/#how-it-works","title":"How It Works","text":"<p>Each chat interface (<code>ChatOpenAI</code>, <code>ChatClaude</code>, and <code>ChatGemini</code>) maintains its own chat history using the <code>ChatHistory</code> class. The history includes all user inputs and model responses, as well as the initial system message.</p>"},{"location":"chat_history/#configuring-chat-history","title":"Configuring Chat History","text":"<p>When initializing a chat interface, you can specify the number of messages to keep in the history:</p> <pre><code>from tinylang.llms import ChatOpenAI\n\nchat = ChatOpenAI(\"gpt-3.5-turbo\", chat_history=5)\n</code></pre> <p>This will keep the last 5 user-model message pairs in the history, in addition to the system message.</p>"},{"location":"chat_history/#accessing-chat-history","title":"Accessing Chat History","text":"<p>You can access the current chat history at any time using the <code>get_messages()</code> method:</p> <pre><code>history = chat.chat_history.get_messages()\nprint(history)\n</code></pre> <p>This returns a list of dictionaries, where each dictionary represents a message with 'role' and 'content' keys.</p>"},{"location":"chat_history/#clearing-chat-history","title":"Clearing Chat History","text":"<p>To clear the chat history:</p> <pre><code>chat.chat_history.clear()\n</code></pre>"},{"location":"chat_history/#using-previous-history","title":"Using Previous History","text":"<p>You can initialize a chat interface with a previous history:</p> <pre><code>previous_history = [\n    {\"role\": \"user\", \"content\": \"Hello, I am Andrew\"},\n    {\"role\": \"assistant\", \"content\": \"Hello Andrew! How can I assist you today?\"},\n]\n\nchat = ChatOpenAI(\"gpt-3.5-turbo\", chat_history=5, previous_history=previous_history)\n</code></pre> <p>This allows you to continue conversations from a previous session or set up specific contexts for your interactions.</p>"},{"location":"chat_history/#accessing-chat-history_1","title":"Accessing Chat History","text":"<p>You can access the current chat history at any time using the get_history() method available in all chat interfaces:</p> <pre><code>history = chat.get_history()\nprint(history)\n</code></pre> <p>This returns a list of dictionaries, where each dictionary represents a message with 'role' and 'content' keys. Alternatively, you can still access the chat history directly through the chat_history attribute:</p> <pre><code>history = chat.chat_history.get_messages()\nprint(history)\n</code></pre> <p>Both methods return the same result.</p>"},{"location":"chat_history/#impact-on-model-responses","title":"Impact on Model Responses","text":"<p>The chat history provides context for the language model, allowing it to generate more relevant and coherent responses across multiple turns of conversation. However, be aware that using a large chat history can increase the token count of your requests, potentially affecting performance and costs.</p>"},{"location":"chat_openai/","title":"ChatOpenAI","text":"<p>The <code>ChatOpenAI</code> class provides an interface to interact with OpenAI's language models, including support for function calling (tools).</p>"},{"location":"chat_openai/#initialization","title":"Initialization","text":"<pre><code>from tinylang.llms import ChatOpenAI\nfrom tinylang.tools import Tool\n\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ntools = [\n    Tool(\n        name=\"multiply\",\n        description=\"Multiply two integers\",\n        function=multiply\n    )\n]\n\nchat = ChatOpenAI(\n    model=\"gpt-3.5-turbo\",\n    api_key=None,  # Optional: Defaults to OPENAI_API_KEY environment variable\n    chat_history=10,  # Optional: Number of messages to keep in history\n    system_message=\"You are a helpful assistant.\",  # Optional: System message for the conversation\n    tools=tools,  # Optional: List of Tool objects for function calling\n    tool_choice=\"auto\"  # Optional: How to choose which function to call\n)\n</code></pre>"},{"location":"chat_openai/#methods","title":"Methods","text":""},{"location":"chat_openai/#invoke","title":"invoke","text":"<p>Synchronous method to get a response from the model.</p> <pre><code>response = chat.invoke(\"What is 5 times 3?\")\nprint(response)\n</code></pre> <p>This might result in the model using the <code>multiply</code> function:</p> <pre><code>To calculate 5 times 3, I'll use the multiply function.\n\n5 times 3 equals 15.\n\nIs there anything else you'd like to know?\n</code></pre>"},{"location":"chat_openai/#ainvoke","title":"ainvoke","text":"<p>Asynchronous version of <code>invoke</code>.</p> <pre><code>import asyncio\n\nasync def get_response():\n    response = await chat.ainvoke(\"What is 7 times 6?\")\n    print(response)\n\nasyncio.run(get_response())\n</code></pre>"},{"location":"chat_openai/#stream_invoke","title":"stream_invoke","text":"<p>Synchronous method that streams the response.</p> <pre><code>for chunk in chat.stream_invoke(\"Calculate 12 times 8 and explain the process.\"):\n    print(chunk, end='', flush=True)\n</code></pre>"},{"location":"chat_openai/#astream_invoke","title":"astream_invoke","text":"<p>Asynchronous version of <code>stream_invoke</code>.</p> <pre><code>async def stream_response():\n    async for chunk in chat.astream_invoke(\"What is 15 times 4? Show your work.\"):\n        print(chunk, end='', flush=True)\n\nasyncio.run(stream_response())\n</code></pre>"},{"location":"chat_openai/#chat-history","title":"Chat History","text":"<p>The <code>ChatOpenAI</code> class automatically manages chat history. You can access the current history using:</p> <pre><code>history = chat.get_history()\nprint(history)\n</code></pre> <p>For more details on chat history management, see the Chat History documentation.</p>"},{"location":"chat_openai/#using-tools-function-calling","title":"Using Tools (Function Calling)","text":"<p>The <code>ChatOpenAI</code> class supports the use of tools (functions) that the model can call when needed. Here's an example of defining and using a more complex tool:</p> <pre><code>from tinylang.llms import ChatOpenAI\nfrom tinylang.tools import Tool\nfrom typing import List\n\ndef calculate_average(numbers: List[float]) -&gt; float:\n    \"\"\"Calculate the average of a list of numbers.\"\"\"\n    return sum(numbers) / len(numbers)\n\ntools = [\n    Tool(\n        name=\"calculate_average\",\n        description=\"Calculate the average of a list of numbers\",\n        function=calculate_average\n    )\n]\n\nchat = ChatOpenAI(\"gpt-3.5-turbo\", tools=tools)\n\nresponse = chat.invoke(\"What's the average of 10, 15, and 20?\")\nprint(response)\n</code></pre> <p>This might result in:</p> <pre><code>To calculate the average of 10, 15, and 20, I'll use the calculate_average function.\n\nThe average of 10, 15, and 20 is 15.\n\nHere's how it works:\n1. The function takes the list of numbers [10, 15, 20].\n2. It calculates the sum of these numbers: 10 + 15 + 20 = 45.\n3. Then it divides the sum by the count of numbers (3 in this case): 45 / 3 = 15.\n\nSo, the average of 10, 15, and 20 is 15.\n\nIs there anything else you'd like to know?\n</code></pre> <p>Remember that the availability and behavior of tools may depend on the specific OpenAI model you're using. Always refer to the latest OpenAI documentation for the most up-to-date information on function calling capabilities.</p>"},{"location":"getting_started/","title":"Getting Started with Tinylang","text":""},{"location":"getting_started/#installation","title":"Installation","text":"<p>Install Tinylang using pip:</p> <pre><code>pip install tinylang\n</code></pre>"},{"location":"getting_started/#configuration","title":"Configuration","text":"<p>Set up your API keys as environment variables:</p> <ul> <li><code>OPENAI_API_KEY</code> for OpenAI</li> <li><code>ANTHROPIC_API_KEY</code> for Claude</li> <li><code>GOOGLE_API_KEY</code> for Gemini</li> </ul> <p>Alternatively, you can pass the API keys directly when initializing the chat interfaces.</p>"},{"location":"getting_started/#basic-usage","title":"Basic Usage","text":"<p>Here's a simple example using the OpenAI interface:</p> <pre><code>from tinylang.llms import ChatOpenAI\n\nchat = ChatOpenAI(\"gpt-3.5-turbo\")\nresponse = chat.invoke(\"Hello, how are you?\")\nprint(response)\n</code></pre> <p>For more examples and detailed usage instructions, check out the documentation for each chat interface:</p> <ul> <li>OpenAI</li> <li>Claude</li> <li>Gemini</li> </ul>"},{"location":"tool_usage/","title":"Tool Usage in Tinylang","text":"<p>Tools in Tinylang allow language models to call specific functions when needed, enhancing their capabilities and allowing for more complex interactions. This document explains how to create, implement, and use tools across different chat interfaces in Tinylang.</p>"},{"location":"tool_usage/#creating-tools","title":"Creating Tools","text":"<p>Tools are created using the <code>Tool</code> class from the <code>tinylang.tools</code> module. Here's the basic structure of a tool:</p> <pre><code>from tinylang.tools import Tool\n\ndef my_function(param1: type1, param2: type2) -&gt; return_type:\n    \"\"\"Function description.\"\"\"\n    # Function implementation\n    return result\n\nmy_tool = Tool(\n    name=\"my_function_name\",\n    description=\"A brief description of what the function does\",\n    function=my_function\n)\n</code></pre> <p>The <code>Tool</code> class automatically infers the function's parameter types and creates an appropriate input model. However, you can also specify a custom input model if needed:</p> <pre><code>from pydantic import BaseModel\n\nclass CustomInputModel(BaseModel):\n    param1: type1\n    param2: type2\n\nmy_tool_with_custom_model = Tool(\n    name=\"my_function_name\",\n    description=\"A brief description of what the function does\",\n    function=my_function,\n    input_model=CustomInputModel\n)\n</code></pre>"},{"location":"tool_usage/#implementing-tools-in-chat-interfaces","title":"Implementing Tools in Chat Interfaces","text":"<p>Currently, tool usage is primarily implemented in the <code>ChatOpenAI</code> class. Here's how to initialize a chat interface with tools:</p> <pre><code>from tinylang.llms import ChatOpenAI\nfrom tinylang.tools import Tool\n\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\ntools = [\n    Tool(\n        name=\"multiply\",\n        description=\"Multiply two integers\",\n        function=multiply\n    )\n]\n\nchat = ChatOpenAI(\n    model=\"gpt-3.5-turbo\",\n    tools=tools,\n    tool_choice=\"auto\"\n)\n</code></pre> <p>The <code>tool_choice</code> parameter determines how the model decides which function to call. Options include:</p> <ul> <li><code>\"auto\"</code>: The model decides whether to call a function and which one to call.</li> <li><code>\"none\"</code>: The model is not allowed to call any functions.</li> <li>A specific tool name: The model is forced to call the specified function.</li> </ul>"},{"location":"tool_usage/#using-tools-in-conversations","title":"Using Tools in Conversations","text":"<p>Once tools are implemented, you can use them in conversations just like normal chat interactions. The model will automatically decide when to use a tool based on the context of the conversation.</p> <pre><code>response = chat.invoke(\"What is 7 times 8?\")\nprint(response)\n</code></pre> <p>This might result in:</p> <pre><code>To calculate 7 times 8, I'll use the multiply function.\n\n7 times 8 equals 56.\n\nIs there anything else you'd like to know?\n</code></pre>"},{"location":"tool_usage/#complex-tool-example","title":"Complex Tool Example","text":"<p>Here's an example of a more complex tool that calculates statistics for a list of numbers:</p> <pre><code>from tinylang.llms import ChatOpenAI\nfrom tinylang.tools import Tool\nfrom typing import List\nimport statistics\n\ndef calculate_stats(numbers: List[float]) -&gt; dict:\n    \"\"\"Calculate various statistics for a list of numbers.\"\"\"\n    return {\n        \"mean\": statistics.mean(numbers),\n        \"median\": statistics.median(numbers),\n        \"stdev\": statistics.stdev(numbers) if len(numbers) &gt; 1 else None\n    }\n\ntools = [\n    Tool(\n        name=\"calculate_stats\",\n        description=\"Calculate mean, median, and standard deviation for a list of numbers\",\n        function=calculate_stats\n    )\n]\n\nchat = ChatOpenAI(\"gpt-3.5-turbo\", tools=tools)\n\nresponse = chat.invoke(\"What are the mean, median, and standard deviation of 10, 15, 20, 25, and 30?\")\nprint(response)\n</code></pre> <p>This might result in:</p> <pre><code>To calculate the statistics for the numbers 10, 15, 20, 25, and 30, I'll use the calculate_stats function.\n\nHere are the results:\n\n1. Mean: 20.0\n   The mean is the average of all numbers. (10 + 15 + 20 + 25 + 30) / 5 = 20.0\n\n2. Median: 20.0\n   The median is the middle number when the list is sorted. In this case, it's 20.\n\n3. Standard Deviation: approximately 7.91\n   The standard deviation measures the amount of variation in the dataset.\n\nThese statistics provide insights into the central tendency (mean and median) and spread (standard deviation) of the given numbers.\n\nIs there anything else you'd like to know about these numbers or any other statistical calculations?\n</code></pre>"},{"location":"tool_usage/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Clear Descriptions: Provide clear and concise descriptions for your tools. This helps the model understand when and how to use them.</p> </li> <li> <p>Type Annotations: Always use type annotations in your function definitions. This allows Tinylang to create accurate input models for the tools.</p> </li> <li> <p>Error Handling: Implement proper error handling in your tool functions. The chat interface will catch and report errors, but well-handled errors provide better user experience.</p> </li> <li> <p>Stateless Functions: Keep your tool functions stateless whenever possible. This ensures consistent behavior across multiple invocations.</p> </li> <li> <p>Testing: Thoroughly test your tools independently before integrating them into chat interfaces.</p> </li> </ol> <p>Remember that the availability and behavior of tools may depend on the specific model you're using. Always refer to the latest documentation of the language model provider for the most up-to-date information on function calling capabilities.</p>"}]}