{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Tinylang","text":"<p>Tinylang is a Python library that provides a unified interface for interacting with various Large Language Models (LLMs) including OpenAI's GPT, Anthropic's Claude, and Google's Gemini.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Unified API for multiple LLM providers</li> <li>Support for OpenAI, Anthropic Claude, and Google Gemini</li> <li>Synchronous and asynchronous invocation methods</li> <li>Streaming support for real-time responses</li> <li>Chat history management</li> <li>Easy integration with existing projects</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install Tinylang:</p> <pre><code>pip install tinylang\n</code></pre> <p>Basic usage:</p> <pre><code>from tinylang.llms import ChatOpenAI\n\nchat = ChatOpenAI(\"gpt-3.5-turbo\")\nresponse = chat.invoke(\"Hello, how are you?\")\nprint(response)\n</code></pre> <p>For more detailed information, check out our Getting Started guide.</p>"},{"location":"api_reference/","title":"API Reference","text":"<p>This page provides a comprehensive reference for the Tinylang API.</p>"},{"location":"api_reference/#chat-interfaces","title":"Chat Interfaces","text":""},{"location":"api_reference/#chatopenai","title":"ChatOpenAI","text":"<pre><code>class ChatOpenAI:\n    def __init__(self, model: str, api_key: Optional[str] = None, chat_history: int = 10, system_message: Optional[str] = None)\n    def invoke(self, prompt: str) -&gt; str\n    async def ainvoke(self, prompt: str) -&gt; str\n    def stream_invoke(self, prompt: str) -&gt; Iterator[str]\n    async def astream_invoke(self, prompt: str) -&gt; AsyncIterable[str]\n    def get_history(self) -&gt; List[Dict[str, str]]\n</code></pre>"},{"location":"api_reference/#chatclaude","title":"ChatClaude","text":"<pre><code>class ChatClaude:\n    def __init__(self, model: str, api_key: Optional[str] = None, chat_history: int = 10, system_message: Optional[str] = None)\n    def invoke(self, prompt: str) -&gt; str\n    async def ainvoke(self, prompt: str) -&gt; str\n    def stream_invoke(self, prompt: str) -&gt; Iterator[str]\n    async def astream_invoke(self, prompt: str) -&gt; AsyncIterable[str]\n    def get_history(self) -&gt; List[Dict[str, str]]\n</code></pre>"},{"location":"api_reference/#chatgemini","title":"ChatGemini","text":"<pre><code>class ChatGemini:\n    def __init__(self, model: str, api_key: Optional[str] = None, chat_history: int = 10, system_message: Optional[str] = None)\n    def invoke(self, prompt: str) -&gt; str\n    async def ainvoke(self, prompt: str) -&gt; str\n    def stream_invoke(self, prompt: str) -&gt; Iterator[str]\n    async def astream_invoke(self, prompt: str) -&gt; AsyncIterable[str]\n    def get_history(self) -&gt; List[Dict[str, str]]\n</code></pre>"},{"location":"api_reference/#chathistory","title":"ChatHistory","text":"<pre><code>class ChatHistory:\n    def __init__(self, max_history: int, system_message: str, previous_history: Optional[List[Dict[str, str]]] = None)\n    def add_message(self, role: str, content: str)\n    def get_messages() -&gt; List[Dict[str, str]]\n    def clear()\n</code></pre> <p>For detailed usage instructions and examples, please refer to the individual documentation pages for each class.</p>"},{"location":"chat_claude/","title":"ChatClaude","text":"<p>The <code>ChatClaude</code> class provides an interface to interact with Anthropic's Claude language models.</p>"},{"location":"chat_claude/#initialization","title":"Initialization","text":"<pre><code>from tinylang.llms import ChatClaude\n\nchat = ChatClaude(\n    model=\"claude-3-opus-20240229\",\n    api_key=None,  # Optional: Defaults to ANTHROPIC_API_KEY environment variable\n    chat_history=10,  # Optional: Number of messages to keep in history\n    system_message=\"You are a helpful assistant.\"  # Optional: System message for the conversation\n)\n</code></pre>"},{"location":"chat_claude/#methods","title":"Methods","text":""},{"location":"chat_claude/#invoke","title":"invoke","text":"<p>Synchronous method to get a response from the model.</p> <pre><code>response = chat.invoke(\"What is the capital of France?\")\nprint(response)\n</code></pre>"},{"location":"chat_claude/#ainvoke","title":"ainvoke","text":"<p>Asynchronous version of <code>invoke</code>.</p> <pre><code>import asyncio\n\nasync def get_response():\n    response = await chat.ainvoke(\"What is the capital of France?\")\n    print(response)\n\nasyncio.run(get_response())\n</code></pre>"},{"location":"chat_claude/#stream_invoke","title":"stream_invoke","text":"<p>Synchronous method that streams the response.</p> <pre><code>for chunk in chat.stream_invoke(\"Tell me a story\"):\n    print(chunk, end='', flush=True)\n</code></pre>"},{"location":"chat_claude/#astream_invoke","title":"astream_invoke","text":"<p>Asynchronous version of <code>stream_invoke</code>.</p> <pre><code>async def stream_response():\n    async for chunk in chat.astream_invoke(\"Tell me a story\"):\n        print(chunk, end='', flush=True)\n\nasyncio.run(stream_response())\n</code></pre>"},{"location":"chat_claude/#chat-history","title":"Chat History","text":"<p>The <code>ChatClaude</code> class automatically manages chat history. You can access the current history using:</p> <pre><code>history = chat.chat_history.get_messages()\nprint(history)\n</code></pre> <p>For more details on chat history management, see the Chat History documentation.</p>"},{"location":"chat_gemini/","title":"ChatGemini","text":"<p>The <code>ChatGemini</code> class provides an interface to interact with Google's Gemini language models.</p>"},{"location":"chat_gemini/#initialization","title":"Initialization","text":"<pre><code>from tinylang.llms import ChatGemini\n\nchat = ChatGemini(\n    model=\"gemini-1.5-pro\",\n    api_key=None,  # Optional: Defaults to GOOGLE_API_KEY environment variable\n    chat_history=10,  # Optional: Number of messages to keep in history\n    system_message=\"You are a helpful assistant.\"  # Optional: System message for the conversation\n)\n</code></pre>"},{"location":"chat_gemini/#methods","title":"Methods","text":""},{"location":"chat_gemini/#invoke","title":"invoke","text":"<p>Synchronous method to get a response from the model.</p> <pre><code>response = chat.invoke(\"What is the capital of France?\")\nprint(response)\n</code></pre>"},{"location":"chat_gemini/#ainvoke","title":"ainvoke","text":"<p>Asynchronous version of <code>invoke</code>.</p> <pre><code>import asyncio\n\nasync def get_response():\n    response = await chat.ainvoke(\"What is the capital of France?\")\n    print(response)\n\nasyncio.run(get_response())\n</code></pre>"},{"location":"chat_gemini/#stream_invoke","title":"stream_invoke","text":"<p>Synchronous method that streams the response.</p> <pre><code>for chunk in chat.stream_invoke(\"Tell me a story\"):\n    print(chunk, end='', flush=True)\n</code></pre>"},{"location":"chat_gemini/#astream_invoke","title":"astream_invoke","text":"<p>Asynchronous version of <code>stream_invoke</code>.</p> <pre><code>async def stream_response():\n    async for chunk in chat.astream_invoke(\"Tell me a story\"):\n        print(chunk, end='', flush=True)\n\nasyncio.run(stream_response())\n</code></pre>"},{"location":"chat_gemini/#chat-history","title":"Chat History","text":"<p>The <code>ChatGemini</code> class automatically manages chat history. You can access the current history using:</p> <pre><code>history = chat.chat_history.get_messages()\nprint(history)\n</code></pre> <p>For more details on chat history management, see the Chat History documentation.</p>"},{"location":"chat_history/","title":"Chat History","text":"<p>Tinylang provides built-in chat history management for all its chat interfaces. This allows for maintaining context across multiple interactions with the language models.</p>"},{"location":"chat_history/#how-it-works","title":"How It Works","text":"<p>Each chat interface (<code>ChatOpenAI</code>, <code>ChatClaude</code>, and <code>ChatGemini</code>) maintains its own chat history using the <code>ChatHistory</code> class. The history includes all user inputs and model responses, as well as the initial system message.</p>"},{"location":"chat_history/#configuring-chat-history","title":"Configuring Chat History","text":"<p>When initializing a chat interface, you can specify the number of messages to keep in the history:</p> <pre><code>from tinylang.llms import ChatOpenAI\n\nchat = ChatOpenAI(\"gpt-3.5-turbo\", chat_history=5)\n</code></pre> <p>This will keep the last 5 user-model message pairs in the history, in addition to the system message.</p>"},{"location":"chat_history/#accessing-chat-history","title":"Accessing Chat History","text":"<p>You can access the current chat history at any time using the <code>get_messages()</code> method:</p> <pre><code>history = chat.chat_history.get_messages()\nprint(history)\n</code></pre> <p>This returns a list of dictionaries, where each dictionary represents a message with 'role' and 'content' keys.</p>"},{"location":"chat_history/#clearing-chat-history","title":"Clearing Chat History","text":"<p>To clear the chat history:</p> <pre><code>chat.chat_history.clear()\n</code></pre>"},{"location":"chat_history/#using-previous-history","title":"Using Previous History","text":"<p>You can initialize a chat interface with a previous history:</p> <pre><code>previous_history = [\n    {\"role\": \"user\", \"content\": \"Hello, I am Andrew\"},\n    {\"role\": \"assistant\", \"content\": \"Hello Andrew! How can I assist you today?\"},\n]\n\nchat = ChatOpenAI(\"gpt-3.5-turbo\", chat_history=5, previous_history=previous_history)\n</code></pre> <p>This allows you to continue conversations from a previous session or set up specific contexts for your interactions.</p>"},{"location":"chat_history/#accessing-chat-history_1","title":"Accessing Chat History","text":"<p>You can access the current chat history at any time using the get_history() method available in all chat interfaces:</p> <pre><code>history = chat.get_history()\nprint(history)\n</code></pre> <p>This returns a list of dictionaries, where each dictionary represents a message with 'role' and 'content' keys. Alternatively, you can still access the chat history directly through the chat_history attribute:</p> <pre><code>history = chat.chat_history.get_messages()\nprint(history)\n</code></pre> <p>Both methods return the same result.</p>"},{"location":"chat_history/#impact-on-model-responses","title":"Impact on Model Responses","text":"<p>The chat history provides context for the language model, allowing it to generate more relevant and coherent responses across multiple turns of conversation. However, be aware that using a large chat history can increase the token count of your requests, potentially affecting performance and costs.</p>"},{"location":"chat_openai/","title":"ChatOpenAI","text":"<p>The <code>ChatOpenAI</code> class provides an interface to interact with OpenAI's language models.</p>"},{"location":"chat_openai/#initialization","title":"Initialization","text":"<pre><code>from tinylang.llms import ChatOpenAI\n\nchat = ChatOpenAI(\n    model=\"gpt-3.5-turbo\",\n    api_key=None,  # Optional: Defaults to OPENAI_API_KEY environment variable\n    chat_history=10,  # Optional: Number of messages to keep in history\n    system_message=\"You are a helpful assistant.\"  # Optional: System message for the conversation\n)\n</code></pre>"},{"location":"chat_openai/#methods","title":"Methods","text":""},{"location":"chat_openai/#invoke","title":"invoke","text":"<p>Synchronous method to get a response from the model.</p> <pre><code>response = chat.invoke(\"What is the capital of France?\")\nprint(response)\n</code></pre>"},{"location":"chat_openai/#ainvoke","title":"ainvoke","text":"<p>Asynchronous version of <code>invoke</code>.</p> <pre><code>import asyncio\n\nasync def get_response():\n    response = await chat.ainvoke(\"What is the capital of France?\")\n    print(response)\n\nasyncio.run(get_response())\n</code></pre>"},{"location":"chat_openai/#stream_invoke","title":"stream_invoke","text":"<p>Synchronous method that streams the response.</p> <pre><code>for chunk in chat.stream_invoke(\"Tell me a story\"):\n    print(chunk, end='', flush=True)\n</code></pre>"},{"location":"chat_openai/#astream_invoke","title":"astream_invoke","text":"<p>Asynchronous version of <code>stream_invoke</code>.</p> <pre><code>async def stream_response():\n    async for chunk in chat.astream_invoke(\"Tell me a story\"):\n        print(chunk, end='', flush=True)\n\nasyncio.run(stream_response())\n</code></pre>"},{"location":"chat_openai/#chat-history","title":"Chat History","text":"<p>The <code>ChatOpenAI</code> class automatically manages chat history. You can access the current history using:</p> <pre><code>history = chat.chat_history.get_messages()\nprint(history)\n</code></pre> <p>For more details on chat history management, see the Chat History documentation.</p>"},{"location":"getting_started/","title":"Getting Started with Tinylang","text":""},{"location":"getting_started/#installation","title":"Installation","text":"<p>Install Tinylang using pip:</p> <pre><code>pip install tinylang\n</code></pre>"},{"location":"getting_started/#configuration","title":"Configuration","text":"<p>Set up your API keys as environment variables:</p> <ul> <li><code>OPENAI_API_KEY</code> for OpenAI</li> <li><code>ANTHROPIC_API_KEY</code> for Claude</li> <li><code>GOOGLE_API_KEY</code> for Gemini</li> </ul> <p>Alternatively, you can pass the API keys directly when initializing the chat interfaces.</p>"},{"location":"getting_started/#basic-usage","title":"Basic Usage","text":"<p>Here's a simple example using the OpenAI interface:</p> <pre><code>from tinylang.llms import ChatOpenAI\n\nchat = ChatOpenAI(\"gpt-3.5-turbo\")\nresponse = chat.invoke(\"Hello, how are you?\")\nprint(response)\n</code></pre> <p>For more examples and detailed usage instructions, check out the documentation for each chat interface:</p> <ul> <li>OpenAI</li> <li>Claude</li> <li>Gemini</li> </ul>"}]}